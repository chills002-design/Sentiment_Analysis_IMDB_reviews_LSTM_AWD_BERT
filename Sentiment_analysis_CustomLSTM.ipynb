{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a25080c8",
   "metadata": {},
   "source": [
    "SENTIMENT ANALYSIS FOR IMDB MOVIE REVIEW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f77c0d",
   "metadata": {},
   "source": [
    "IMPORT THE NECESSARY LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b64885d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All libraries imported successfully!\n",
      "ðŸ“Š Pandas version: 2.3.3\n"
     ]
    }
   ],
   "source": [
    "## Create custom LSTM model for sentiment analysis of IMDB movie reviews from Kaggle using Pytorch\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset # for loading datasets from Hugging Face \n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(\"ðŸ“Š Pandas version:\", pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "177cd042",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ad4c71",
   "metadata": {},
   "source": [
    "LOAD THE DATASET FROM KAGGLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "460cef74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully! Shape: (25000, 2)\n",
      "                                                text  label\n",
      "0  I rented I AM CURIOUS-YELLOW from my video sto...      0\n",
      "1  \"I Am Curious: Yellow\" is a risible and preten...      0\n",
      "2  If only to avoid making this type of film in t...      0\n",
      "3  This film was probably inspired by Godard's Ma...      0\n",
      "4  Oh, brother...after hearing about this ridicul...      0\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"stanfordnlp/imdb\")\n",
    "df = pd.DataFrame(ds['train'])\n",
    "print(\"Dataset loaded successfully! Shape:\", df.shape)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf944481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Preprocessing function defined successfully!\n"
     ]
    }
   ],
   "source": [
    "## preprocessing dataset\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Remove expanded contractions\n",
    "    contraction_mapping = {\"can't\": \"cannot\", \n",
    "                           \"won't\": \"will not\", \n",
    "                           \"n't\": \" not\", \n",
    "                           \"'re\": \" are\", \n",
    "                           \"'s\": \" is\", \n",
    "                           \"'d\": \" would\", \n",
    "                           \"'ll\": \" will\"}\n",
    "    for contraction, expansion in contraction_mapping.items():\n",
    "        text = re.sub(r\"\\b{}\\b\".format(contraction), expansion, text)\n",
    "    return text\n",
    "\n",
    "print(\"âœ… Preprocessing function defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fbfe3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tokenization and vocabulary building functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "## Tokenization and Vocabulary Building\n",
    "\n",
    "MAX_VOCAB = 20000\n",
    "MIN_FREQ = 2\n",
    "MAXLEN = 256\n",
    "EMBED_DIM = 128\n",
    "HIDDEN_DIM = 128\n",
    "OUT_DIM = 1\n",
    "NUM_LAYERS = 1\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "DROP_OUT = 0.5\n",
    "PAD_IDX = 1\n",
    "UNK_IDX = 1\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "def build_vocab(sentences, min_freq=2):\n",
    "    token_counter = Counter()\n",
    "    for sentence in sentences:\n",
    "        tokens = sentence.split()\n",
    "        token_counter.update(tokens)\n",
    "    vocab = {token: idx + 2 for idx, (token, freq) in enumerate(token_counter.items()) if freq >= min_freq}\n",
    "    vocab['<PAD>'] = 0\n",
    "    vocab['<UNK>'] = 1\n",
    "    return vocab\n",
    "\n",
    "def encode_sentence(sentence, vocab, maxlen=200):\n",
    "    tokens = sentence.split()\n",
    "    encoded = [vocab.get(token, UNK_IDX) for token in tokens]\n",
    "    if len(encoded) < maxlen:\n",
    "        encoded += [PAD_IDX] * (maxlen - len(encoded))\n",
    "    else:\n",
    "        encoded = encoded[:maxlen]\n",
    "    return encoded\n",
    "print(\"âœ… Tokenization and vocabulary building functions defined successfully!\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f02d444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Vocabulary built successfully! Vocabulary size: 56175\n",
      "âœ… Reviews encoded successfully!\n"
     ]
    }
   ],
   "source": [
    "df['review'] = df['text'].apply(preprocess_text) ## preprocess reviews\n",
    "vocab = build_vocab(df['review'], min_freq=MIN_FREQ) ## build vocabulary    \n",
    "print(\"âœ… Vocabulary built successfully! Vocabulary size:\", len(vocab))\n",
    "\n",
    "df['encoded_review'] = df['review'].apply(lambda x: encode_sentence(x, vocab, maxlen=MAXLEN)) ## encode reviews\n",
    "print(\"âœ… Reviews encoded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d35b5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data split into train, validation, and test sets successfully!\n",
      "Train size: 18000 Validation size: 2000 Test size: 5000\n"
     ]
    }
   ],
   "source": [
    "## Split data into train, validation, and test sets\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.1, random_state=42, stratify=train_data['label'])\n",
    "print(\"âœ… Data split into train, validation, and test sets successfully!\")\n",
    "print(\"Train size:\", len(train_data), \"Validation size:\", len(val_data), \"Test size:\", len(test_data))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01f34a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Custom Dataset class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "class IMDBDataset(Dataset): ## custom dataset class\n",
    "    def __init__(self, data): ## initialize with data\n",
    "        self.reviews = data['encoded_review'].tolist() ## list of encoded reviews\n",
    "        self.labels = data['label'].tolist() ## list of labels\n",
    "    \n",
    "    def __len__(self): ## length of dataset\n",
    "        return len(self.reviews) ## return length of reviews\n",
    "    \n",
    "    def __getitem__(self, idx): ## get item by index\n",
    "        return torch.tensor(self.reviews[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.float) ## return review and label as tensors\n",
    "print(\"âœ… Custom Dataset class defined successfully!\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e1e8056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DataLoaders created successfully!\n"
     ]
    }
   ],
   "source": [
    "## Create DataLoaders\n",
    "train_dataset = IMDBDataset(train_data)\n",
    "val_dataset = IMDBDataset(val_data)\n",
    "test_dataset = IMDBDataset(test_data)   \n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "print(\"âœ… DataLoaders created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4a2d98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LSTM model class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "## Define LSTM Model\n",
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, out_dim, num_layers, drop_out):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=PAD_IDX)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=drop_out)\n",
    "        self.fc = nn.Linear(hidden_dim, out_dim)\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "        hidden = self.dropout(hidden[-1])\n",
    "        out = self.fc(hidden)\n",
    "        return self.sigmoid(out).squeeze()\n",
    "print(\"âœ… LSTM model class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0319f86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model instantiated and moved to device: mps\n",
      "âœ… Loss function and optimizer defined successfully!\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab) ## vocabulary size\n",
    "model = SentimentLSTM(vocab_size, EMBED_DIM, HIDDEN_DIM, OUT_DIM, NUM_LAYERS, DROP_OUT) ## instantiate model\n",
    "model = model.to(device) ## move model to device\n",
    "print(\"âœ… Model instantiated and moved to device:\", device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "print(\"âœ… Loss function and optimizer defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23831564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training and evaluation functions defined successfully!\n",
      "Epoch 1/10 | Train Loss: 0.6944 | Train Acc: 0.5023 | Val Loss: 0.6941 | Val Acc: 0.4985\n",
      "Epoch 2/10 | Train Loss: 0.6934 | Train Acc: 0.5032 | Val Loss: 0.6938 | Val Acc: 0.4990\n",
      "Epoch 3/10 | Train Loss: 0.6931 | Train Acc: 0.5057 | Val Loss: 0.6936 | Val Acc: 0.4980\n",
      "Epoch 4/10 | Train Loss: 0.6927 | Train Acc: 0.5082 | Val Loss: 0.6934 | Val Acc: 0.5025\n",
      "Epoch 5/10 | Train Loss: 0.6925 | Train Acc: 0.5111 | Val Loss: 0.6933 | Val Acc: 0.4975\n",
      "Epoch 6/10 | Train Loss: 0.6922 | Train Acc: 0.5157 | Val Loss: 0.6932 | Val Acc: 0.4990\n",
      "Epoch 7/10 | Train Loss: 0.6919 | Train Acc: 0.5171 | Val Loss: 0.6932 | Val Acc: 0.4990\n",
      "Epoch 8/10 | Train Loss: 0.6917 | Train Acc: 0.5192 | Val Loss: 0.6931 | Val Acc: 0.5000\n",
      "Epoch 9/10 | Train Loss: 0.6914 | Train Acc: 0.5202 | Val Loss: 0.6930 | Val Acc: 0.5025\n",
      "Epoch 10/10 | Train Loss: 0.6912 | Train Acc: 0.5209 | Val Loss: 0.6930 | Val Acc: 0.5030\n",
      "âœ… Model training completed!\n",
      "Test Loss: 0.6925 | Test Acc: 0.5168\n",
      "âœ… Model evaluation on test set completed!\n"
     ]
    }
   ],
   "source": [
    "## Training and Evaluation Functions\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        for reviews, labels in train_loader:\n",
    "            reviews, labels = reviews.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(reviews)\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            preds = (predictions >= 0.5).float()\n",
    "            epoch_acc += (preds == labels).sum().item()\n",
    "        epoch_loss /= len(train_loader)\n",
    "        epoch_acc /= len(train_loader.dataset)\n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, criterion)\n",
    "        print(f'Epoch {epoch+1}/{epochs} | Train Loss: {epoch_loss:.4f} | Train Acc: {epoch_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}')\n",
    "\n",
    "## Model Evaluation      \n",
    "def evaluate_model(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for reviews, labels in data_loader:\n",
    "            reviews, labels = reviews.to(device), labels.to(device)\n",
    "            predictions = model(reviews)\n",
    "            loss = criterion(predictions, labels)\n",
    "            epoch_loss += loss.item()\n",
    "            preds = (predictions >= 0.5).float()\n",
    "            epoch_acc += (preds == labels).sum().item()\n",
    "    epoch_loss /= len(data_loader)\n",
    "    epoch_acc /= len(data_loader.dataset)\n",
    "    return epoch_loss, epoch_acc\n",
    "print(\"âœ… Training and evaluation functions defined successfully!\")\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, EPOCHS)\n",
    "print(\"âœ… Model training completed!\")\n",
    "test_loss, test_acc = evaluate_model(model, test_loader, criterion)\n",
    "print(f'Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}')\n",
    "print(\"âœ… Model evaluation on test set completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7c83bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55444869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.51      0.84      0.64      2500\n",
      "    positive       0.55      0.19      0.28      2500\n",
      "\n",
      "    accuracy                           0.52      5000\n",
      "   macro avg       0.53      0.52      0.46      5000\n",
      "weighted avg       0.53      0.52      0.46      5000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2112  388]\n",
      " [2028  472]]\n"
     ]
    }
   ],
   "source": [
    "classification_report and confusion_matrix\n",
    "def get_predictions(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for reviews, labels in data_loader:\n",
    "            reviews = reviews.to(device)\n",
    "            predictions = model(reviews)\n",
    "            preds = (predictions >= 0.5).float().cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.numpy())\n",
    "    return np.array(all_preds), np.array(all_labels)\n",
    "test_preds, test_labels = get_predictions(model, test_loader)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(test_labels, test_preds, target_names=['negative', 'positive']))\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8d1e41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: I absolutely loved this movie! The acting was superb and the story was touching.\n",
      "Processed: i absolutely loved this movie the acting was superb and the story was touching\n",
      "Predicted sentiment: negative (prob=0.4971)\n"
     ]
    }
   ],
   "source": [
    "# Single-example prediction\n",
    "example = \"I absolutely loved this movie! The acting was superb and the story was touching.\"\n",
    "processed = preprocess_text(example)\n",
    "encoded = encode_sentence(processed, vocab, maxlen=MAXLEN)\n",
    "input_tensor = torch.tensor(encoded, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prob = model(input_tensor).item()\n",
    "\n",
    "label = 'positive' if prob >= 0.5 else 'negative'\n",
    "print(f\"Review: {example}\")\n",
    "print(f\"Processed: {processed}\")\n",
    "print(f\"Predicted sentiment: {label} (prob={prob:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2b7005",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
